
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Resume &#8212; HomePage-WangXin  documentation</title>
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/language_data.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Research" href="research.html" />
    <link rel="prev" title="Welcome" href="index.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="resume">
<span id="label-resume"></span><h1>Resume<a class="headerlink" href="#resume" title="Permalink to this headline">¶</a></h1>
<p>Here is the <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AABWT45G1hu8XuHtwF7pqaLAa/web/resume.pdf?raw=1">resume in PDF</a>.</p>
<p>My <a class="reference external" href="https://scholar.google.com/citations?user=uMZhUHcAAAAJ&amp;hl=en">Google Scholar page</a> and <a class="reference external" href="https://researchmap.jp/wangxin?lang=en">Researchmap site</a>.</p>
<div class="section" id="basic-info">
<h2>Basic info<a class="headerlink" href="#basic-info" title="Permalink to this headline">¶</a></h2>
<p><strong>Xin Wang</strong></p>
<p>Project assistant professor (in fact, post-doc)</p>
<p>National Institute of Informatics</p>
<p>2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan</p>
</div>
<div class="section" id="education">
<h2>Education<a class="headerlink" href="#education" title="Permalink to this headline">¶</a></h2>
<p>Ph.D: 2015 - 2018 National Institute of Informatics, SOKENDAI, Tokyo, Japan.</p>
<blockquote>
<div><p>Fundamental frequency modeling for neural-betwork-based statistical parametric speech synthesis</p>
<p>Supervisor: Prof. Junichi Yamagishi</p>
<ul class="simple">
<li><p>Thesis (submitted 2018-06-29): <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACVs-tg32gsREezFhoQC1vAa/web/XinWang_THESIS_v0809.pdf?raw=1">PDF version</a></p></li>
<li><p>Slides for thesis defense: <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AAAU462lbZN8qgwiTPlv8dvEa/web/THESIS_E4.pdf?raw=1">defense slides</a></p></li>
<li><p>Appendix: <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACemZwy4AU8UvssM0rI5dn6a/web/THESIS_appendix_highway.pdf?raw=1">highway network</a>, <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACdQpbXtMTt7j7--C8UQqUGa/web/THESIS_appendix_sar.pdf?raw=1">SAR</a>, <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AABQpYIevQ50TVAcm5kWumdwa/web/THESIS_appendix_dar.pdf?raw=1">DAR</a>, <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AADPEvEa56UsxDHMgxPnddnGa/web/THESIS_appendix_vqvae.pdf?raw=1">VQ-VAE</a></p></li>
</ul>
</div></blockquote>
<p>M.Sc.: 2012 - 2015 University of Science and Technology of China, Hefei, China.</p>
<blockquote>
<div><p>Bi-directional optimization for concept-to-speech synthesis</p>
<p>Supervisor: Prof. Zhen-Hua Ling</p>
</div></blockquote>
<p>B.Sc.: 2008 - 2012 University of Electronic Science and Technology of China, Chengdu, China.</p>
</div>
<div class="section" id="academic-activity">
<h2>Academic activity<a class="headerlink" href="#academic-activity" title="Permalink to this headline">¶</a></h2>
<p>Organizer</p>
<blockquote>
<div><ul class="simple">
<li><p>ISCA <a class="reference external" href="https://www.asvspoof.org/">ASVspoof challenges</a> 2021, 2019</p></li>
<li><p>ISCA Interspeech 2020 special session on <a class="reference external" href="https://www.voiceprivacychallenge.org">voice privacy</a></p></li>
<li><p>APSIPA ASC 2019 special session on <a class="reference external" href="http://apsipa2019.org/SpecialSessions.html">Deep Generative Models for Media Clones and Its Detection</a></p></li>
<li><p>ISCA Interspeech 2019 special session on <a class="reference external" href="https://www.interspeech2019.org/program/special_sessions_and_challenges">Automatic Speaker Verification Spoofing and Countermeasures Challenge 2019 (ASVSpoof 2019)</a></p></li>
<li><p>IEEE ASRU 2019 special session on <a class="reference external" href="http://asru2019.org/wp/?page_id=622">ASVspoof 2019</a></p></li>
</ul>
</div></blockquote>
<p>Guest editor</p>
<blockquote>
<div><ul class="simple">
<li><p>Computer Speech and Language <a class="reference external" href="https://www.journals.elsevier.com/computer-speech-and-language/call-for-papers/advances-in-automatic-speaker">Special issue on Advances in Automatic Speaker Verification Anti-spoofing</a></p></li>
</ul>
</div></blockquote>
<p>Reviewer</p>
<blockquote>
<div><ul class="simple">
<li><p>IEEE/ACM TASLP, TBIOM, Signal processing letters, ICASSP, EUSIPCO</p></li>
<li><p>Elsevier Computer speech &amp; language</p></li>
<li><p>ISCA Interspeech, Speech synthesis workshop</p></li>
<li><p>IEICE Trans on Information and Systems</p></li>
</ul>
</div></blockquote>
<dl class="simple">
<dt>Chair</dt><dd><ul class="simple">
<li><p>Interspeech 2021</p></li>
</ul>
</dd>
</dl>
</div>
<div class="section" id="grants">
<h2>Grants<a class="headerlink" href="#grants" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>2021 - 2023, Speech privacy protection by high-quality, invertible, and extendable speech anonymization and de-anonymization, JSPS, Wakate, 21K17775, investigator: Xin Wang</p></li>
<li><p>2020 - 2021, Deep-learning-based neural source-filtering models for fast and high-quality music signal generation, KAWAI, investigator: Xin Wang</p></li>
<li><p>2019 - 2021, One model for all sounds: fast and high-quality neural source-filter model for speech and non-speech waveform modeling, JSPS, grant-for-startup, 19K24371, investigator: Xin Wang</p></li>
<li><p>2019 - 2020, AI Focused Research Awards Program in Japan: Robust and all-purpose neural source-filter models, Google, investigators: Junichi Yamagishi, Xin Wang, Eric Cooper</p></li>
</ul>
</div>
<div class="section" id="publication">
<h2>Publication<a class="headerlink" href="#publication" title="Permalink to this headline">¶</a></h2>
<p>Journal</p>
<ol class="arabic simple">
<li><p><strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2020. Neural Source-Filter Waveform Models for Statistical Parametric Speech Synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing 28: 402–415. doi:10.1109/TASLP.2019.2956145.</p></li>
<li><p><strong>Xin Wang</strong>, Shinji Takaki, Junichi Yamagishi, Simon King, and Keiichi Tokuda. 2020. A Vector Quantized Variational Autoencoder (VQ-VAE) Autoregressive Neural F0 Model for Statistical Parametric Speech Synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing 28: 157–170. doi:10.1109/TASLP.2019.2950099.</p></li>
<li><p><strong>Xin Wang</strong>, Junichi Yamagishi, Massimiliano Todisco, Hector Delgado, Andreas Nautsch, Nicholas Evans, Md Sahidullah, Ville Vestman, Tomi Kinnunen, Kong Aik Lee, Lauri Juvela, Paavo Alku, Yu-Huai Peng, Hsin-Te Hwang, Yu Tsao, Hsin-Min Wang, Sébastien Le Maguer, Markus Becker, Fergus Henderson, Rob Clark, Yu Zhang, Quan Wang, Ye Jia, Kai Onuma, Koji Mushika, Takashi Kaneda, Yuan Jiang, Li-Juan Liu, Yi-Chiao Wu, Wen-Chin Huang, Tomoki Toda, Kou Tanaka, Hirokazu Kameoka, Ingmar Steiner, Driss Matrouf, Jean-François Bonastre, Avashna Govender, Srikanth Ronanki, Jing-Xuan Zhang, and Zhen-Hua Ling. 2020. ASVspoof 2019: A Large-Scale Public Database of Synthesized, Converted and Replayed Speech. Computer Speech &amp; Language, 101114. doi:https://doi.org/10.1016/j.csl.2020.101114.</p></li>
<li><p><strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2018. Investigating Very Deep Highway Networks for Parametric Speech Synthesis. Speech Communication 96: 1–9. doi:https://doi.org/10.1016/j.specom.2017.11.002.</p></li>
<li><p><strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2018. Autoregressive Neural F0 Model for Statistical Parametric Speech Synthesis. IEEE/ACM Transactions on Audio, Speech, and Language Processing 26 (8). IEEE: 1406–1419. doi:10.1109/TASLP.2018.2828650.</p></li>
<li><p><strong>Xin Wang</strong>, Zhen-Hua Ling, and Li-Rong Dai. 2016. Concept-to-Speech Generation with Knowledge Sharing for Acoustic Modelling and Utterance Filtering. Computer Speech &amp; Language 38 (July). Elsevier: 46–67. doi:10.1016/j.csl.2015.12.003.</p></li>
<li><p><strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2016. Investigation of Using Continuous Representation of Various Linguistic Units in Neural Network Based Text-to-Speech Synthesis. IEICE Transactions on Information and Systems E99.D (10): 2471–2480. doi:10.1587/transinf.2016SLP0011.</p></li>
<li><p>Andreas Nautsch, <strong>Xin Wang</strong>, Nicolas Evans, Tomi Kinnunen, Ville Vestman, Massimiliano Todisco, Hector Delgado, Md Sahidullah, Junichi Yamagishi, and Kong Aik Lee.  ASVspoof 2019: Spoofing Countermeasures for the Detection of Synthesized, Converted and Replayed Speech . IEEE Trans. on Biometrics, Behavior, and Identity (T-BIOM), (accepted). 2021.</p></li>
<li><p>Yusuke Yasuda, <strong>Xin Wang</strong>, and Junichi Yamagishi.  Investigation of Learning Abilities on Linguistic Features in Sequence-to-Sequence Text-to-Speech Synthesis . Computer Speech &amp; Language 67: 101183doi:<a class="reference external" href="https://doi.org/10.1016/j.csl.2020.101183">https://doi.org/10.1016/j.csl.2020.101183</a>. 2021.</p></li>
<li><p>Shuhei Kato, Yusuke Yasuda, <strong>Xin Wang</strong>, Erica Cooper, Shinji Takaki, and Junichi Yamagishi. 2020. Modeling of Rakugo Speech and Its Limitations: Toward Speech Synthesis That Entertains Audiences. IEEE Access 8: 138149–138161.</p></li>
<li><p>Tomi Kinnunen, Hector Delgado, Nicholas Evans, Kong Aik Lee, Ville Vestman, Andreas Nautsch, Massimiliano Todisco, <strong>Xin Wang</strong>, Md Sahidullah, Junichi Yamagishi, and Douglas A Reynolds. 2020. Tandem Assessment of Spoofing Countermeasures and Automatic Speaker Verification: Fundamentals. IEEE/ACM Transactions on Audio, Speech, and Language Processing. IEEE.</p></li>
</ol>
<p>Conference</p>
<ol class="arabic simple">
<li><p>Lin Zhang, <strong>Xin Wang</strong>, Erica Cooper and Junichi Yamagishi. Multi-task learning in utterance-level and segmental-level spoof detection. In Proc. ISCA ASVspoof workshop, (accepted) 2021</p></li>
<li><p>Junichi Yamagishi, <strong>Xin Wang</strong>, Massimiliano Todisco, Md Sahidullah, Jose Patino, Andreas Nautsch, Xuechen Liu, Kong Aik Lee, Tomi Kinnunen, Nicholas Evans, Hector Delgado. ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection. In Proc. ISCA ASVspoof workshop, (accepted) 2021</p></li>
<li><p><strong>Xin Wang</strong>, Junich Yamagishi. Comparative Study on Recent Neural Spoofing Countermeasures for Synthetic Speech Detection. In Proc. Interspeech, (accepted) 2021.</p></li>
<li><p>Lin Zhang, <strong>Xin Wang</strong>, Erica Cooper, Junichi Yamagishi, Jose Patino and Nicholas Evans, An Initial Investigation for Detecting Partially Spoofed Audio. In Proc. Interspeech, (accepted) 2021.</p></li>
<li><p>Tomi Kinnunen, Andreas Nautsch, Md. Sahidullah, Nicholas Evans, <strong>Xin Wang</strong>, Massimiliano Todisco, Héctor Delgado, Junichi Yamagishi and Kong Aik Lee. Visualizing Classifier Adjacency Relations: A Case Study in Speaker Verification and Voice Anti-Spoofing. In Proc. Interspeech, (accepted) 2021.</p></li>
<li><p>Erica Cooper, <strong>Xin Wang</strong> and Junichi Yamagishi. Text-to-Speech Synthesis Techniques for MIDI-to-Audio Synthesis. In Proc. SSW (accepted) 2021.</p></li>
<li><p>Yang Ai, Haoyu Li, <strong>Xin Wang</strong>, Junichi Yamagishi, and Zhenhua Ling.  Denoising-and-Dereverberation Hierarchical Neural Vocoder for Robust Waveform Generation . In Proc. SLT, (to be published). 2021.</p></li>
<li><p>Yusuke Yasuda, <strong>Xin Wang</strong>, and Junichi Yamagishi.  End-to-End Text-to-Speech Using Latent Duration Based on VQ-VAE . In Proc. ICASSP, (accepted). 2021.</p></li>
<li><p>Shuhei Kato, Yusuke Yasuda, <strong>Xin Wang</strong>, Erica Cooper, and Junichi Yamagishi.  How Similar or Different Is Rakugo Speech Synthesizer to Professional Performers? . In Proc. ICASSP, (accepted). 2021.</p></li>
<li><p>Brij Mohan Lal Srivastava, Natalia Tomashenko, <strong>Xin Wang</strong>, Emmanuel Vincent, Junichi Yamagishi, Mohamed Maouche, Aurelien Bellet, and Marc Tommasi. 2020. Design Choices for X-Vector Based Speaker Anonymization. In Proc. Interspeech, 1713-1717. ISCA: ISCA. doi:10.21437/Interspeech.2020-2692.</p></li>
<li><p>Natalia Tomashenko, Brij Mohan Lal Srivastava, <strong>Xin Wang</strong>, Emmanuel Vincent, Andreas Nautsch, Junichi Yamagishi, Nicholas Evans, Jose Patino, Jean-Francois Bonastre, Paul-Gauthier Noe, and Massimiliano Todisco. 2020. Introducing the VoicePrivacy Initiative. In Proc. Interspeech, 1693-1697. ISCA: ISCA. doi:10.21437/Interspeech.2020-1333.</p></li>
<li><p><strong>Xin Wang</strong>, and Junichi Yamagishi. 2020. Using Cyclic Noise as the Source Signal for Neural Source-Filter-Based Speech Waveform Model. In Proc. Interspeech, 1992-1996. ISCA: ISCA. doi:10.21437/Interspeech.2020-1018.</p></li>
<li><p>Yang Ai, <strong>Xin Wang</strong>, Junichi Yamagishi, and Zhen-Hua Ling. 2020. Reverberation Modeling for Source-Filter-Based Neural Vocoder. In Proc. Interspeech, 3560-3564. doi:10.21437/Interspeech.2020-1613.</p></li>
<li><p>Yi Zhao, <strong>Xin Wang</strong>, Lauri Juvela, and Junichi Yamagishi. 2020. Transferring Neural Speech Waveform Synthesizers to Musical Instrument Sounds Generation. In Proc. ICASSP, 6269–6273. IEEE. doi:10.1109/ICASSP40776.2020.9053047.</p></li>
<li><p>Yusuke Yasuda, <strong>Xin Wang</strong>, and Junichi Yamagishi. 2020. Effect of Choice of Probability Distribution, Randomness, and Search Methods for Alignment Modeling in Sequence-to-Sequence Text-to-Speech Synthesis Using Hard Alignment. In Proc. ICASSP, 6724–6728.</p></li>
<li><p>Erica Cooper, Cheng-I Lai, Yusuke Yasuda, Fuming Fang, <strong>Xin Wang</strong>, Nanxin Chen, and Junichi Yamagishi. 2020. Zero-Shot Multi-Speaker Text-to-Speech with State-of-the-Art Neural Speaker Embeddings. In Proc. ICASSP, 6184–6188.</p></li>
<li><p>Mingyang Zhang, <strong>Xin Wang</strong>, Fuming Fang, Haizhou Li, and Junichi Yamagishi. 2019. Joint Training Framework for Text-to-Speech and Voice Conversion Using Multi-Source Tacotron and WaveNet. In Proc. Interspeech, 1298–1302. doi:10.21437/Interspeech.2019-1357.</p></li>
<li><p>Yusuke Yasuda, <strong>Xin Wang</strong>, and Junichi Yamagishi. 2019. Initial Investigation of Encoder-Decoder End-to-End TTS Using Marginalization of Monotonic Hard Alignments. In Proc. SSW, 211–216. doi:10.21437/SSW.2019-38.</p></li>
<li><p>Yusuke Yasuda, <strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2019. Investigation of Enhanced Tacotron Text-to-Speech Synthesis Systems with Self-Attention for Pitch Accent Language. In Proc. ICASSP, 6905–6909.</p></li>
<li><p><strong>Xin Wang</strong>, and Junichi Yamagishi. 2019. Neural Harmonic-plus-Noise Waveform Model with Trainable Maximum Voice Frequency for Text-to-Speech Synthesis. In Proc. SSW, 1–6. ISCA: ISCA. doi:10.21437/SSW.2019-1.</p></li>
<li><p><strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2019. Neural Source-Filter-Based Waveform Model for Statistical Parametric Speech Synthesis. In Proc. ICASSP, 5916–5920.</p></li>
<li><p>Massimiliano Todisco, <strong>Xin Wang</strong>, Ville Vestman, Md. Sahidullah, Héctor Delgado, Andreas Nautsch, Junichi Yamagishi, Nicholas Evans, Tomi H Kinnunen, and Kong Aik Lee. 2019. ASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection. In Proc. Interspeech, 1008–1012. doi:10.21437/Interspeech.2019-2249.</p></li>
<li><p>Shinji Takaki, Toru Nakashika, <strong>Xin Wang</strong>, and Junichi Yamagishi. 2019. STFT Spectral Loss for Training a Neural Speech Waveform Model. In Proc. ICASSP, 7065–7069.</p></li>
<li><p>Hieu-Thi Luong, <strong>Xin Wang</strong>, Junichi Yamagishi, and Nobuyuki Nishizawa. 2019. Training Multi-Speaker Neural Text-to-Speech Systems Using Speaker-Imbalanced Speech Corpora. In Proc. Interspeech, 1303–1307. doi:10.21437/Interspeech.2019-1311.</p></li>
<li><p>Chen-Chou Lo, Szu-Wei Fu, Wen-Chin Huang, <strong>Xin Wang</strong>, Junichi Yamagishi, Yu Tsao, and Hsin-Min Wang. 2019. MOSnet: Deep Learning-Based Objective Assessment for Voice Conversion. In Proc. Interspeech, 1541–1545. doi:10.21437/Interspeech.2019-2003.</p></li>
<li><p>Shuhei Kato, Yusuke Yasuda, <strong>Xin Wang</strong>, Erica Cooper, Shinji Takaki, and Junichi Yamagishi. 2019. Rakugo Speech Synthesis Using Segment-to-Segment Neural Transduction and Style Tokens - toward Speech Synthesis for Entertaining Audiences. In Proc. SSW, 111–116. doi:10.21437/SSW.2019-20.</p></li>
<li><p>Fuming Fang, <strong>Xin Wang</strong>, Junichi Yamagishi, Isao Echizen, Massimiliano Todisco, Nicholas Evans, and Jean-Francois Bonastre. 2019. Speaker Anonymization Using X-Vector and Neural Waveform Models. In Proc. SSW, 155–160. doi:10.21437/SSW.2019-28.</p></li>
<li><p>Fuming Fang, <strong>Xin Wang</strong>, Junichi Yamagishi, and Isao Echizen. 2019. Audiovisual Speaker Conversion: Jointly and Simultaneously Transforming Facial Expression and Acoustic Characteristics. In Proc. ICASSP, 6795–6799.</p></li>
<li><p><strong>Xin Wang</strong>, Jaime Lorenzo-Trueba, Shinji Takaki, Lauri Juvela, and Junichi Yamagishi. 2018. A Comparison of Recent Waveform Generation and Acoustic Modeling Methods for Neural-Network-Based Speech Synthesis. In Proc. ICASSP, 4804–4808.</p></li>
<li><p>Hieu-Thi Luong, <strong>Xin Wang</strong>, Junichi Yamagishi, and Nobuyuki Nishizawa. 2018. Investigating Accuracy of Pitch-Accent Annotations in Neural-Network-Based Speech Synthesis and Denoising Effects. In Proc. Interspeech 2018, 37–41.</p></li>
<li><p>Jaime Lorenzo-Trueba, Fuming Fang, <strong>Xin Wang</strong>, Isao Echizen, Junichi Yamagishi, and Tomi Kinnunen. 2018. Can We Steal Your Vocal Identity from the Internet?: Initial Investigation of Cloning Obama’s Voice Using GAN, WaveNet and Low-Quality Found Data. In Proc. Odyssey, 240–247. ISCA: ISCA. doi:10.21437/Odyssey.2018-34.</p></li>
<li><p>Lauri Juvela, Bajibabu Bollepalli, <strong>Xin Wang</strong>, Hirokazu Kameoka, Manu Airaksinen, Junichi Yamagishi, and Paavo Alku. 2018. Speech Waveform Synthesis from MFCC Sequences with Generative Adversarial Networks. In Proc. ICASSP, 5679–5683.</p></li>
<li><p>Gustav Eje Henter, Jaime Lorenzo-Trueba, <strong>Xin Wang</strong>, Mariko Kondo, and Junichi Yamagishi. 2018. Cyborg Speech: Deep Multilingual Speech Synthesis for Generating Segmental Foreign Accent with Natural Prosody. In Proc. ICASSP, 4799–4803.</p></li>
<li><p><strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2017. An RNN-Based Quantized F0 Model with Multi-Tier Feedback Links for Text-to-Speech Synthesis. In Proc. Interspeech, 1059–1063.</p></li>
<li><p><strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2017. An Autoregressive Recurrent Mixture Density Network for Parametric Speech Synthesis. In Proc. ICASSP, 4895–4899.</p></li>
<li><p>Gustav Eje Henter, Jaime Lorenzo-Trueba, <strong>Xin Wang</strong>, and Junichi Yamagishi. 2017. Principles for Learning Controllable TTS from Annotated and Latent Variation. In Proc. Interspeech, 3956–3960. doi:10.21437/Interspeech.2017-171.</p></li>
<li><p><strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2016. Enhance the Word Vector with Prosodic Information for the Recurrent Neural Network Based TTS System. In Proc. Interspeech, 2856–2860.</p></li>
<li><p><strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2016. Investigating Very Deep Highway Networks for Parametric Speech Synthesis. In Proc. SSW9, 181–186.</p></li>
<li><p><strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2016. A Comparative Study of the Performance of HMM, DNN, and RNN Based Speech Synthesis Systems Trained on Very Large Speaker-Dependent Corpora. In Proc. SSW9, 125–128.</p></li>
<li><p><strong>Xin Wang</strong>, Minghui Dong, and Zhenhua Ling. 2016. A Full Training Framework of Cross-Stream Dependence Modelling for HMM-Based Singing Voice Synthesis. In Proc. ICASSP, 5165–5169. doi:10.1109/ICASSP.2016.7472662.</p></li>
<li><p>Cassia Valentini-Botinhao, <strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2016. Speech Enhancement for a Noise-Robust Text-to-Speech Synthesis System Using Deep Recurrent Neural Networks. In Proc. Interspeech, 352–356.</p></li>
<li><p>Cassia Valentini-Botinhao, <strong>Xin Wang</strong>, Shinji Takaki, and Junichi Yamagishi. 2016. Investigating RNN-Based Speech Enhancement Methods for Noise-Robust Text-to-Speech. In Proc. SSW, 146–152.</p></li>
<li><p>Lauri Juvela, <strong>Xin Wang</strong>, Shinji Takaki, Manu Airaksinen, Junichi Yamagishi, and Paavo Alku. 2016. Using Text and Acoustic Features in Predicting Glottal Excitation Waveforms for Parametric Speech Synthesis with Recurrent Neural Networks. In Proc. Interspeech, 2283–2287.</p></li>
<li><p><strong>Xin Wang</strong>, Zhen-Hua Ling, and Li-Rong Dai. 2014. Concept-to-Speech Generation by Integrating Syntagmatic Features into HMM-Based Speech Synthesis. In Proc. Interspeech, 2942–2946.</p></li>
<li><p><strong>Xin Wang</strong>, Zhen-Hua Ling, and Li-Rong Dai. 2012. Cross-Stream Dependency Modeling Using Continuous F0 Model for HMM-Based Speech Synthesis. In Proc. ISCSLP, 84–87.</p></li>
</ol>
</div>
<div class="section" id="talk">
<h2>Talk<a class="headerlink" href="#talk" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>2021 July, <code class="docutils literal notranslate"><span class="pre">Advancement</span> <span class="pre">in</span> <span class="pre">Neural</span> <span class="pre">Vocoders</span></code>. Tutorial at ISCA 2021 Speech Processing Courses in Crete, with Prof. Yamagishi. Hands-on-materials on <a class="reference external" href="https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts/tree/master/tutorials">github</a>. <a class="reference external" href="https://www.slideshare.net/jyamagis/advancements-in-neural-vocoders">Slides</a></p></li>
<li><p>2020 Nov., <code class="docutils literal notranslate"><span class="pre">Neural</span> <span class="pre">statistical</span> <span class="pre">parametric</span> <span class="pre">speech</span> <span class="pre">synthesis</span></code>. Tutorial as ISCA 2020 Speaker Odyssey, Tokyo. <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AABFY0RiorILzSuX1YuQXyA7a/web/Odyssesy2020_Tutorial_TTS_XINWANG.pdf?raw=1">PDF</a> and <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AABn3DyzRuZeBJwEGPV1ouFSa/web/Odyssesy2020_Tutorial_TTS_XINWANG.pptx?raw=1">PPT slides</a> are available.</p></li>
<li><p>2020 July, <code class="docutils literal notranslate"><span class="pre">Neural</span> <span class="pre">auto-regressive,</span> <span class="pre">source-filter</span> <span class="pre">and</span> <span class="pre">glottal</span> <span class="pre">vocoders</span> <span class="pre">for</span> <span class="pre">speech</span> <span class="pre">and</span> <span class="pre">music</span> <span class="pre">signals</span></code>. Tutorial at ISCA 2020 Speech Processing Courses in Crete, with Prof. Yamagishi. Hands-on-materials on <a class="reference external" href="https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts/tree/master/tutorials">github</a>.</p></li>
<li><p>2019 Sep, <code class="docutils literal notranslate"><span class="pre">Neural</span> <span class="pre">waveform</span> <span class="pre">models</span> <span class="pre">for</span> <span class="pre">text-to-speech</span> <span class="pre">synthesis</span></code>, Fraunhofer IIS, invited talk, Erlangen, Germany. Slide is <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AAByUSX6u4O51bGHpIFlgy-ba/web/201909-FraunhoderIIS-neural-waveform-models.pdf?raw=1">here 1</a></p></li>
<li><p>2019 Jan, <code class="docutils literal notranslate"><span class="pre">Tutorial</span> <span class="pre">on</span> <span class="pre">recent</span> <span class="pre">neural</span> <span class="pre">waveform</span> <span class="pre">models</span></code>, IEICE Technical Committee on Speech (SP), invited tutorial, Kanazawa, Japan. Slide is <a class="reference external" href="https://www.slideshare.net/jyamagis/tutorial-on-endtoend-texttospeech-synthesis-part-1-neural-waveform-modeling">here 2</a></p></li>
<li><p>2018 Nov, <code class="docutils literal notranslate"><span class="pre">Autoregressive</span> <span class="pre">neural</span> <span class="pre">networks</span> <span class="pre">for</span> <span class="pre">parametric</span> <span class="pre">speech</span> <span class="pre">synthesis</span></code>, Nagoya Institute of Technology, Tokuda lab. Slide is <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACZVX1Tf9Qw1MUc2YHQKf4Ia/web/20180111-Nagoya-ARmodels.pdf?raw=1">here 3</a></p></li>
<li><p>2018 Jun, <code class="docutils literal notranslate"><span class="pre">Autoregressive</span> <span class="pre">neural</span> <span class="pre">networks</span> <span class="pre">for</span> <span class="pre">parametric</span> <span class="pre">speech</span> <span class="pre">synthesis</span></code>, University of Eastern Finland, School of Computing, Aalto University, Paavo Alku lab (same content as above)</p></li>
</ul>
</div>
<div class="section" id="awards-scholarship">
<h2>Awards &amp; scholarship<a class="headerlink" href="#awards-scholarship" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>Best paper award for <a class="reference external" href="https://www.soken.ac.jp/news/5935/">SSW 2016</a>, ISCA SynSig</p></li>
<li><p><a class="reference external" href="https://www.soken.ac.jp/news/5935/">SOKENDAI Award</a>, SOKENDAI, Japan</p></li>
<li><p><a class="reference external" href="https://www.ieice.org/iss/sp/jpn/special/sp-prize.html">Young Researcher’s Award in Speech Field</a>, IEICE ISS, Japan</p></li>
<li><p>11th IEEE Signal Processing Society Japan Student <a class="reference external" href="https://ieee-jp.org/section/tokyo/chapter/SP-01/past-student-paper.htm">Best Paper Award</a>, IEEE Japan</p></li>
<li><p>MEXT Scholarship (Ph.D 2015 - 2018), Japan</p></li>
</ul>
<div class="toctree-wrapper compound">
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Page contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Resume</a><ul>
<li><a class="reference internal" href="#basic-info">Basic info</a></li>
<li><a class="reference internal" href="#education">Education</a></li>
<li><a class="reference internal" href="#academic-activity">Academic activity</a></li>
<li><a class="reference internal" href="#grants">Grants</a></li>
<li><a class="reference internal" href="#publication">Publication</a></li>
<li><a class="reference internal" href="#talk">Talk</a></li>
<li><a class="reference internal" href="#awards-scholarship">Awards &amp; scholarship</a></li>
</ul>
</li>
</ul>
<h3><a href="index.html">Site map</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Welcome</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Resume</a></li>
<li class="toctree-l1"><a class="reference internal" href="research.html">Research work</a></li>
<li class="toctree-l1"><a class="reference internal" href="slide.html">Talk &amp; slides</a></li>
</ul>

<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, WangXin.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.2.1</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/resume.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>