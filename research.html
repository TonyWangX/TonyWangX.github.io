<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Research &#8212; Home-page-WangXin  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/sphinx_highlight.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="JST PRESTO Project" href="presto.html" />
    <link rel="prev" title="Resume" href="resume.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="research">
<span id="label-research"></span><h1>Research<a class="headerlink" href="#research" title="Permalink to this heading">¶</a></h1>
<section id="ph-d">
<h2>Ph.D<a class="headerlink" href="#ph-d" title="Permalink to this heading">¶</a></h2>
<p>My PhD thesis focuses on F0 modeling. It includes an investigation using highway network, detailed explanation about many aspects of autoregressive (AR) models, and new models based on variational autoencoder (VAE).</p>
<p>Title: Fundamental Frequency Modeling for Neural-Network-Based Statistical Parametric Speech Synthesis</p>
<ul class="simple">
<li><p>Thesis (submitted 2018-06-29): <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACVs-tg32gsREezFhoQC1vAa/web/XinWang_THESIS_v0809.pdf?raw=1">thesis PDF version</a></p></li>
<li><p>Slides for thesis defense: <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AAAU462lbZN8qgwiTPlv8dvEa/web/THESIS_E4.pdf?raw=1">defense slides</a></p></li>
<li><p>Appendix: <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACemZwy4AU8UvssM0rI5dn6a/web/THESIS_appendix_highway.pdf?raw=1">highway network</a>, <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACdQpbXtMTt7j7--C8UQqUGa/web/THESIS_appendix_sar.pdf?raw=1">SAR</a>, <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AABQpYIevQ50TVAcm5kWumdwa/web/THESIS_appendix_dar.pdf?raw=1">DAR</a>, <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AADPEvEa56UsxDHMgxPnddnGa/web/THESIS_appendix_vqvae.pdf?raw=1">VQ-VAE</a></p></li>
</ul>
<p>Many details and results are not reported in the thesis. Please check appendix.</p>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</section>
<section id="speech-anti-spoofing">
<h2>Speech Anti-spoofing<a class="headerlink" href="#speech-anti-spoofing" title="Permalink to this heading">¶</a></h2>
<section id="database">
<h3>Database<a class="headerlink" href="#database" title="Permalink to this heading">¶</a></h3>
<section id="asvspoof-2019">
<h4>ASVspoof 2019<a class="headerlink" href="#asvspoof-2019" title="Permalink to this heading">¶</a></h4>
<a class="reference internal image-reference" href="_images/LA_TABLE1.png"><img alt="_images/LA_TABLE1.png" src="_images/LA_TABLE1.png" style="height: 200px;" /></a>
<p>A large scale database with bona fide (real) and spoofing (fake) audios from many advanced TTS or VC systems. It publically available:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.asvspoof.org/index2019.html">ASVspoof 2019 website</a></p></li>
<li><p><a class="reference external" href="https://doi.org/10.7488/ds/2555">Database link on Datashare</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/1911.01601">Description on the database</a></p></li>
<li><p><a class="reference external" href="https://nii-yamagishilab.github.io/samples-xin/main-asvspoof2019.html">Audio samples for the LA subset</a></p></li>
</ul>
</section>
<section id="asvspoof-2021">
<h4>ASVspoof 2021<a class="headerlink" href="#asvspoof-2021" title="Permalink to this heading">¶</a></h4>
<p>Database for ASVspoof 2021, including:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://doi.org/10.5281/zenodo.4837263">ASVspoof 2021 LA database</a>: a new LA test set with codec and transmission effects</p></li>
<li><p><a class="reference external" href="https://doi.org/10.5281/zenodo.4834716">ASVspoof 2021 PA database</a>: a new PA test set recoded in real room environments</p></li>
<li><p><a class="reference external" href="https://doi.org/10.5281/zenodo.4835108">ASVspoof 2021 DF database</a>: a new trakc called DF, with various audios from various data sources and audio compression effects</p></li>
<li><p><a class="reference external" href="https://github.com/asvspoof-challenge/2021">Baseline code for ASVspoof 2021</a></p></li>
<li><p>Labels are available <a class="reference external" href="https://www.asvspoof.org">here</a></p></li>
<li><p>Conference proceeding for <a class="reference external" href="https://www.isca-speech.org/archive/asvspoof_2021/index.html">ASVspoof 2021 workshop</a></p></li>
</ul>
</section>
</section>
<section id="countermeasures">
<h3>Countermeasures<a class="headerlink" href="#countermeasures" title="Permalink to this heading">¶</a></h3>
<section id="comparison-on-baseline-cms">
<h4>Comparison on baseline CMs<a class="headerlink" href="#comparison-on-baseline-cms" title="Permalink to this heading">¶</a></h4>
<a class="reference internal image-reference" href="_images/fig_eer_table.png"><img alt="_images/fig_eer_table.png" src="_images/fig_eer_table.png" style="height: 200px;" /></a>
<p>A comparison of recent neural spoofing countermeasures on ASVspoof2019 LA database. Pre-trained models and training recipes are available:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts">github link</a>. Check project/03-asvspoof-mega</p></li>
<li><p>Interspeech 2021 presentation <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AAAbQM0rKGea4t5i5m6rn_F_a/web/2021-interspeech-Fri-M-V-7-1.pdf?raw=1">PPT</a> and <a class="reference external" href="https://www.isca-speech.org/archive/interspeech_2021/wang21fa_interspeech.html">Paper link</a></p></li>
<li><p>A <a class="reference external" href="https://arxiv.org/abs/2201.03321">new book chapter</a> summarizing the common countermeasures.</p></li>
</ul>
</section>
<section id="countermeasure-with-confidence-estimator">
<h4>Countermeasure with confidence estimator<a class="headerlink" href="#countermeasure-with-confidence-estimator" title="Permalink to this heading">¶</a></h4>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/nii-yamagishilab/project-NN-Pytorch-scripts/master/misc/Conf-estimator_test2.png"><img alt="https://raw.githubusercontent.com/nii-yamagishilab/project-NN-Pytorch-scripts/master/misc/Conf-estimator_test2.png" src="https://raw.githubusercontent.com/nii-yamagishilab/project-NN-Pytorch-scripts/master/misc/Conf-estimator_test2.png" style="height: 300px;" /></a>
<p>Adding an confidence estimator to the countermeasure so that we can know how confident the CM is.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts">github link</a>. Check project/06-asvspoof-ood</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2110.04775">Arxiv Paper</a></p></li>
</ul>
</section>
<section id="countermeasure-using-ssl-based-front-end">
<h4>Countermeasure using SSL-based front end<a class="headerlink" href="#countermeasure-using-ssl-based-front-end" title="Permalink to this heading">¶</a></h4>
<a class="reference internal image-reference" href="https://raw.githubusercontent.com/nii-yamagishilab/project-NN-Pytorch-scripts/master/misc/fig-ssl.png"><img alt="https://raw.githubusercontent.com/nii-yamagishilab/project-NN-Pytorch-scripts/master/misc/fig-ssl.png" src="https://raw.githubusercontent.com/nii-yamagishilab/project-NN-Pytorch-scripts/master/misc/fig-ssl.png" style="height: 200px;" /></a>
<p>Instead of using conventional DSP-based features (e.g., LFCC), using self-supervised model can extract more robust features. It works very well on different test sets.</p>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts">github link</a>. Check project/07-asvspoof-ssl</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2111.07725">paper</a> on Arxiv</p></li>
</ul>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</section>
</section>
</section>
<section id="nsf-model">
<h2>NSF model<a class="headerlink" href="#nsf-model" title="Permalink to this heading">¶</a></h2>
<a class="reference internal image-reference" href="_images/nsf.png"><img alt="_images/nsf.png" src="_images/nsf.png" style="height: 200px;" /></a>
<p>A non-autoregressive neural source-filter waveform model that performs reasonably well in terms of speech quality but faster on generation speed. No distilling is needed for model training.</p>
<p>Here is the <a class="reference external" href="https://nii-yamagishilab.github.io/samples-nsf/">Home page of NSF model</a>.</p>
<div class="line-block">
<div class="line"><br /></div>
<div class="line"><br /></div>
</div>
</section>
<section id="text-to-speech">
<h2>Text-to-speech<a class="headerlink" href="#text-to-speech" title="Permalink to this heading">¶</a></h2>
<p>My own work on text-to-speech (TTS) is on classical statistical parametric speech synthesis.</p>
<section id="tts-comparison">
<h3>TTS comparison<a class="headerlink" href="#tts-comparison" title="Permalink to this heading">¶</a></h3>
<a class="reference internal image-reference" href="_images/tts_systems.png"><img alt="_images/tts_systems.png" src="_images/tts_systems.png" style="height: 250px;" /></a>
<p>This work compares recent acoustic models and waveform generators: SAR and DAR refers to the acoustic models on this page below; WORLD and PML are waveform generators. This was published in ICASSP 2018</p>
<ul class="simple">
<li><p>It was presented at <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AAC8XgykCv9hSChQMgtzAmVSa/web/2018-ICASSP.pdf?raw=1">slide at ICASSP 2018</a></p></li>
<li><p>Tool: WaveNet in CURRENNT is implemented on CUDA/Thrust. Code for <a class="reference external" href="https://github.com/nii-yamagishilab/project-CURRENNT-scripts">WaveNet</a></p></li>
</ul>
</section>
<section id="deep-ar-f0-model">
<h3>Deep AR F0 model<a class="headerlink" href="#deep-ar-f0-model" title="Permalink to this heading">¶</a></h3>
<a class="reference internal image-reference" href="_images/F0MODEL.png"><img alt="_images/F0MODEL.png" src="_images/F0MODEL.png" style="height: 300px;" /></a>
<p>An old idea for machine learning and signal processing but not well investigated for neural-network-based speech synthesis. The motivation is that RNN may not model the temporal correlation of the target feature sequence as we expect. A simple technique to examine the capability of the model is to draw samples from the model. On this aspect, none of the previous model can generate good F0 contours by random sampling.</p>
<ul class="simple">
<li><p>It is detailed in this journal paper (<a class="reference external" href="https://ieeexplore.ieee.org/document/8341752/">open access</a>)</p></li>
<li><p>It is also detailed in <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACVs-tg32gsREezFhoQC1vAa/web/XinWang_THESIS_v0809.pdf?raw=1">Ph.D thesis (ch.6)</a></p></li>
<li><p>More details are in <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AAAU462lbZN8qgwiTPlv8dvEa/web/THESIS_E4.pdf?raw=1">Ph.D defense slides</a></p></li>
<li><p>It was presented at <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AAA0rZJEq6lQYU98mamyterka/web/2017-interspeech.pdf?raw=1">Interspeech 2018</a></p></li>
</ul>
</section>
<section id="shallow-ar-acoustic-model">
<h3>Shallow AR acoustic model<a class="headerlink" href="#shallow-ar-acoustic-model" title="Permalink to this heading">¶</a></h3>
<a class="reference internal image-reference" href="_images/sar.png"><img alt="_images/sar.png" src="_images/sar.png" style="height: 250px;" /></a>
<p>RNN is a special recurrent mixture density network (RMDN). While both networks use recurrent layers, neither captures the temporal correlation of the target features. Similar to the F0 model above, autoregressive links can be used to amend the missing temporal correlation. This model is called shallow because it only uses linear transformation to model the AR dependency.</p>
<p>I like this model becaue it is related to signal processing, linear-prediction, normalization flow …</p>
<ul class="simple">
<li><p>It is detailed in <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACVs-tg32gsREezFhoQC1vAa/web/XinWang_THESIS_v0809.pdf?raw=1">Ph.D thesis (ch.5)</a></p></li>
<li><p>More details are in <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AAAU462lbZN8qgwiTPlv8dvEa/web/THESIS_E4.pdf?raw=1">Ph.D defense slides</a></p></li>
<li><p>It was presented at <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AAA5syHnVZvJrljcOILi5U4ga/web/2017-ICASSP.pdf?raw=1">ICASSP 2017</a></p></li>
</ul>
</section>
<section id="id4">
<h3>Highway Network<a class="headerlink" href="#id4" title="Permalink to this heading">¶</a></h3>
<a class="reference internal image-reference" href="_images/Highway.png"><img alt="_images/Highway.png" src="_images/Highway.png" style="height: 250px;" /></a>
<p>This work investigates highway network for speech synthesis and got some interesting observations:</p>
<ol class="arabic simple">
<li><p>The accuracy of the generated spectral features can be improved consistently as the depth of the multi-stream highway network was increased from 2 to 40</p></li>
<li><p>F0 can be generated equally well by either a deep or a shallow network in the multi-stream network</p></li>
<li><p>The single-stream network must be large enough to model both spectral and F0 features well</p></li>
<li><p>Analysis on the histogram of the highway gates supports the above observations</p></li>
</ol>
<p>Here are some materials for reference:</p>
<ul class="simple">
<li><p>It is detailed in <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACVs-tg32gsREezFhoQC1vAa/web/XinWang_THESIS_v0809.pdf?raw=1">Ph.D thesis (ch.4)</a></p></li>
<li><p>More details are in <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AAAU462lbZN8qgwiTPlv8dvEa/web/THESIS_E4.pdf?raw=1">Ph.D defense slides</a></p></li>
<li><p>It was published as a journal paper on <a class="reference external" href="https://www.sciencedirect.com/science/article/pii/S0167639316303703">Speech Communication</a></p></li>
</ul>
</section>
<section id="another-comparison">
<h3>Another comparison<a class="headerlink" href="#another-comparison" title="Permalink to this heading">¶</a></h3>
<a class="reference internal image-reference" href="_images/f009_mushra_.jpg"><img alt="_images/f009_mushra_.jpg" src="_images/f009_mushra_.jpg" style="height: 200px;" /></a>
<a class="reference internal image-reference" href="_images/m007_mushra_.jpg"><img alt="_images/m007_mushra_.jpg" src="_images/m007_mushra_.jpg" style="height: 200px;" /></a>
<p>This initial work tries to show the influence of the amount of training data on the performance of the speech synthesis system. Two large Japanese corpora were utilized (female 50 hours, male 100 hours). The result indicates that we can not benefit from (blindly) increasing the amount of training data for spectral feature modelling.</p>
<ul class="simple">
<li><p>It is presented at <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AACozQp08QjxkmyFEDQlMDZha/web/2016_JVoice.pdf?raw=1">SSW 2016</a></p></li>
</ul>
</section>
</section>
<section id="prosody-embedding">
<h2>Prosody Embedding<a class="headerlink" href="#prosody-embedding" title="Permalink to this heading">¶</a></h2>
<a class="reference internal image-reference" href="_images/prosody_emb.png"><img alt="_images/prosody_emb.png" src="_images/prosody_emb.png" style="height: 150px;" /></a>
<p>This work consists of two parts:</p>
<ul class="simple">
<li><p>Among embedded vectors of phoneme, syllable, and phrase, the phrase vector can be useful for feed-forward network. For recurrent network, not so useful;</p></li>
<li><p>Word vectors can be enhanced by prosodic information extracted from a ToBI annotation task.</p></li>
</ul>
<p>Here are materials:</p>
<ul class="simple">
<li><p>The paper was published in <a class="reference external" href="https://www.dropbox.com/sh/gf3zp00qvdp3row/AADDYHrpFe6b8AbjWjqpRuqTa/web/2016-interspeech.pdf?raw=1">Interspeech 2016</a></p></li>
<li><p>Also in <a class="reference external" href="https://www.jstage.jst.go.jp/article/transinf/E99.D/10/E99.D_2016SLP0011/_article">IEICE journal</a></p></li>
</ul>
</section>
<section id="toolkit">
<h2>Toolkit<a class="headerlink" href="#toolkit" title="Permalink to this heading">¶</a></h2>
<section id="modified-currennt">
<h3>Modified CURRENNT<a class="headerlink" href="#modified-currennt" title="Permalink to this heading">¶</a></h3>
<a class="reference internal image-reference" href="_images/currennt.png"><img alt="_images/currennt.png" src="_images/currennt.png" style="height: 250px;" /></a>
<p>Most of my work was doen using CURRENNT, a CUDA/Thrust toolkit for neural networks. I made intensive revision and added many functions as the above figure shows. It is slow but enjoyable experience to write forward and backward propagation of each neurons through CUDA/Thrust.</p>
<p>The <a class="reference external" href="https://github.com/nii-yamagishilab/project-CURRENNT-public">modified toolkit</a> and <a class="reference external" href="https://github.com/nii-yamagishilab/project-CURRENNT-scripts">utility scripts</a> are on github.</p>
<p>There are a few slides where I summarized my understanding on coding with CUDA/Thrust. Please check the <a class="reference internal" href="slide.html#label-slide"><span class="std std-ref">Talk &amp; slides</span></a>.</p>
<p>Here is the <a class="reference external" href="https://www.jmlr.org/papers/v16/weninger15a.html">original CURRENNT toolkit</a>.</p>
</section>
<section id="pytorch-project">
<h3>Pytorch Project<a class="headerlink" href="#pytorch-project" title="Permalink to this heading">¶</a></h3>
<p>Recently I started to use Pytorch. The first project is to re-implement NSF models using Pytorch.</p>
<p>This project comes with demonstration and tutorials: <a class="reference external" href="https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts">https://github.com/nii-yamagishilab/project-NN-Pytorch-scripts</a></p>
<div class="toctree-wrapper compound">
</div>
</section>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="index.html">Page contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Research</a><ul>
<li><a class="reference internal" href="#ph-d">Ph.D</a></li>
<li><a class="reference internal" href="#speech-anti-spoofing">Speech Anti-spoofing</a><ul>
<li><a class="reference internal" href="#database">Database</a><ul>
<li><a class="reference internal" href="#asvspoof-2019">ASVspoof 2019</a></li>
<li><a class="reference internal" href="#asvspoof-2021">ASVspoof 2021</a></li>
</ul>
</li>
<li><a class="reference internal" href="#countermeasures">Countermeasures</a><ul>
<li><a class="reference internal" href="#comparison-on-baseline-cms">Comparison on baseline CMs</a></li>
<li><a class="reference internal" href="#countermeasure-with-confidence-estimator">Countermeasure with confidence estimator</a></li>
<li><a class="reference internal" href="#countermeasure-using-ssl-based-front-end">Countermeasure using SSL-based front end</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#nsf-model">NSF model</a></li>
<li><a class="reference internal" href="#text-to-speech">Text-to-speech</a><ul>
<li><a class="reference internal" href="#tts-comparison">TTS comparison</a></li>
<li><a class="reference internal" href="#deep-ar-f0-model">Deep AR F0 model</a></li>
<li><a class="reference internal" href="#shallow-ar-acoustic-model">Shallow AR acoustic model</a></li>
<li><a class="reference internal" href="#id4">Highway Network</a></li>
<li><a class="reference internal" href="#another-comparison">Another comparison</a></li>
</ul>
</li>
<li><a class="reference internal" href="#prosody-embedding">Prosody Embedding</a></li>
<li><a class="reference internal" href="#toolkit">Toolkit</a><ul>
<li><a class="reference internal" href="#modified-currennt">Modified CURRENNT</a></li>
<li><a class="reference internal" href="#pytorch-project">Pytorch Project</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3><a href="index.html">Site map</a></h3>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Welcome</a></li>
<li class="toctree-l1"><a class="reference internal" href="resume.html">Resume</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Research overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="presto.html">Research PRESTO</a></li>
<li class="toctree-l1"><a class="reference internal" href="slide.html">Talk &amp; slides</a></li>
</ul>

<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2025, WangXin.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 7.0.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/research.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>